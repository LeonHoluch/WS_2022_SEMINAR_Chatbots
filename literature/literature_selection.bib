@InProceedings{Young2002,
  author    = {Steve Young},
  booktitle = {7th International Conference on Spoken Language Processing ({ICSLP} 2002)},
  title     = {Talking to machines (statistically speaking)},
  year      = {2002},
  month     = {sep},
  publisher = {{ISCA}},
  comment   = {TOD System},
  doi       = {10.21437/icslp.2002-2},
}

@InProceedings{Stent2004,
  author    = {Stent, Amanda and Prasad, Rashmi and Walker, Marilyn},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04)},
  title     = {Trainable Sentence Planning for Complex Information Presentations in Spoken Dialog Systems},
  year      = {2004},
  address   = {Barcelona, Spain},
  month     = jul,
  pages     = {79--86},
  comment   = {NLG},
  doi       = {10.3115/1218955.1218966},
  url       = {https://aclanthology.org/P04-1011},
}

@InProceedings{Boyer2010,
  author    = {Boyer, Kristy and Ha, Eun Y. and Phillips, Robert and Wallis, Michael and Vouk, Mladen and Lester, James},
  booktitle = {Proceedings of the {SIGDIAL} 2010 Conference},
  title     = {Dialogue Act Modeling in a Complex Task-Oriented Domain},
  year      = {2010},
  address   = {Tokyo, Japan},
  month     = sep,
  pages     = {297--305},
  publisher = {Association for Computational Linguistics},
  comment   = {Dialogue Act Modeling},
  file      = {:Boyer2010.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/W10-4356},
}

@InProceedings{Ritter2011,
  author    = {Ritter, Alan and Cherry, Colin and Dolan, William B.},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  title     = {Data-Driven Response Generation in Social Media},
  year      = {2011},
  address   = {Edinburgh, Scotland, UK.},
  month     = jul,
  pages     = {583--593},
  publisher = {Association for Computational Linguistics},
  comment   = {Response Generation (not only TOD)},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/D11-1054},
}

@InProceedings{Henderson2013,
  author    = {Henderson, Matthew and Thomson, Blaise and Young, Steve},
  booktitle = {Proceedings of the {SIGDIAL} 2013 Conference},
  title     = {Deep Neural Network Approach for the Dialog State Tracking Challenge},
  year      = {2013},
  address   = {Metz, France},
  month     = aug,
  pages     = {467--471},
  publisher = {Association for Computational Linguistics},
  comment   = {Dialogue State Tracking},
  file      = {:Henderson2013.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/W13-4073},
}

@Article{Vinyals2015,
  author        = {Oriol Vinyals and Quoc Le},
  title         = {A Neural Conversational Model},
  year          = {2015},
  month         = jun,
  abstract      = {Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.},
  archiveprefix = {arXiv},
  comment       = {Response Generation (not only TOD)},
  eprint        = {1506.05869},
  groups        = {TaskOrientedDialogue},
  keywords      = {cs.CL},
  primaryclass  = {cs.CL},
  url           = {http://arxiv.org/pdf/1506.05869v3},
}

@Article{Mesnil2015,
  author  = {Mesnil, Gr√©goire and Dauphin, Yann and Yao, Kaisheng and Bengio, Yoshua and Deng, Li and Hakkani-Tur, Dilek and He, Xiaodong and Heck, Larry and Tur, Gokhan and Yu, Dong and Zweig, Geoffrey},
  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  title   = {Using Recurrent Neural Networks for Slot Filling in Spoken Language Understanding},
  year    = {2015},
  number  = {3},
  pages   = {530-539},
  volume  = {23},
  comment = {Slot Filling},
  doi     = {10.1109/TASLP.2014.2383614},
}

@InProceedings{Andreas2016,
  author    = {Andreas, Jacob and Klein, Dan},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  title     = {Reasoning about Pragmatics with Neural Listeners and Speakers},
  year      = {2016},
  address   = {Austin, Texas},
  month     = nov,
  pages     = {1173--1182},
  publisher = {Association for Computational Linguistics},
  comment   = {Modelling Reference Games (edge case for dialogue, but task-oriented)},
  doi       = {10.18653/v1/D16-1125},
  url       = {https://aclanthology.org/D16-1125},
}

@InProceedings{Liu2016,
  author    = {Bing Liu and Ian Lane},
  booktitle = {Interspeech 2016},
  title     = {Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling},
  year      = {2016},
  month     = {sep},
  publisher = {{ISCA}},
  comment   = {Intent Detection, Slot Filling},
  doi       = {10.21437/interspeech.2016-1352},
}

@InProceedings{Mrksic2017,
  author    = {Mrk{\v{s}}i{\'c}, Nikola and {\'O} S{\'e}aghdha, Diarmuid and Wen, Tsung-Hsien and Thomson, Blaise and Young, Steve},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  title     = {Neural Belief Tracker: Data-Driven Dialogue State Tracking},
  year      = {2017},
  address   = {Vancouver, Canada},
  month     = jul,
  pages     = {1777--1788},
  publisher = {Association for Computational Linguistics},
  abstract  = {One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user{'}s goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users{'} language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.},
  comment   = {Dialogue State Tracking},
  doi       = {10.18653/v1/P17-1163},
  file      = {:Mrksic2017.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/P17-1163},
}

@Article{Yan2017,
  author    = {Zhao Yan and Nan Duan and Peng Chen and Ming Zhou and Jianshe Zhou and Zhoujun Li},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {Building Task-Oriented Dialogue Systems for Online Shopping},
  year      = {2017},
  month     = feb,
  number    = {1},
  volume    = {31},
  comment   = {TOD System, Application},
  doi       = {10.1609/aaai.v31i1.11182},
  file      = {:Yan2017.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}

@InProceedings{Larsson2017,
  author    = {Larsson, Staffan},
  booktitle = {Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue},
  title     = {User-initiated Sub-dialogues in State-of-the-art Dialogue Systems},
  year      = {2017},
  address   = {Saarbr{\"u}cken, Germany},
  month     = aug,
  pages     = {17--22},
  publisher = {Association for Computational Linguistics},
  abstract  = {We test state of the art dialogue systems for their behaviour in response to user-initiated sub-dialogues, i.e. interactions where a system question is responded to with a question or request from the user, who thus initiates a sub-dialogue. We look at sub-dialogues both within a single app (where the sub-dialogue concerns another topic in the original domain) and across apps (where the sub-dialogue concerns a different domain). The overall conclusion of the tests is that none of the systems can be said to deal appropriately with user-initiated sub-dialogues.},
  comment   = {analysis},
  doi       = {10.18653/v1/W17-5503},
  url       = {https://aclanthology.org/W17-5503},
}

@InProceedings{Hoehn2017,
  author    = {H{\"o}hn, Sviatlana},
  booktitle = {Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue},
  title     = {A data-driven model of explanations for a chatbot that helps to practice conversation in a foreign language},
  year      = {2017},
  address   = {Saarbr{\"u}cken, Germany},
  month     = aug,
  pages     = {395--405},
  publisher = {Association for Computational Linguistics},
  abstract  = {This article describes a model of other-initiated self-repair for a chatbot that helps to practice conversation in a foreign language. The model was developed using a corpus of instant messaging conversations between German native and non-native speakers. Conversation Analysis helped to create computational models from a small number of examples. The model has been validated in an AIML-based chatbot. Unlike typical retrieval-based dialogue systems, the explanations are generated at run-time from a linguistic database.},
  comment   = {modelling other-initiated self-repair for practicing conversation in foreign languages},
  doi       = {10.18653/v1/W17-5547},
  url       = {https://aclanthology.org/W17-5547},
}

@InProceedings{Wei2018,
  author    = {Wei, Zhongyu and Liu, Qianlong and Peng, Baolin and Tou, Huaixiao and Chen, Ting and Huang, Xuanjing and Wong, Kam-fai and Dai, Xiangying},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  title     = {Task-oriented Dialogue System for Automatic Diagnosis},
  year      = {2018},
  address   = {Melbourne, Australia},
  month     = jul,
  pages     = {201--207},
  publisher = {Association for Computational Linguistics},
  abstract  = {In this paper, we make a move to build a dialogue system for automatic diagnosis. We first build a dataset collected from an online medical forum by extracting symptoms from both patients{'} self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make diagnosis for patients automatically, which can converse with patients to collect additional symptoms beyond their self-reports. Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis.},
  comment   = {TOD System, Application},
  doi       = {10.18653/v1/P18-2033},
  file      = {:Wei2018.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/P18-2033},
}

@InProceedings{Ramanarayanan2018,
  author    = {Ramanarayanan, Vikram and Pugh, Robert},
  booktitle = {Proceedings of the 19th Annual {SIG}dial Meeting on Discourse and Dialogue},
  title     = {Automatic Token and Turn Level Language Identification for Code-Switched Text Dialog: An Analysis Across Language Pairs and Corpora},
  year      = {2018},
  address   = {Melbourne, Australia},
  month     = jul,
  pages     = {80--88},
  publisher = {Association for Computational Linguistics},
  abstract  = {We examine the efficacy of various feature{--}learner combinations for language identification in different types of text-based code-switched interactions {--} human-human dialog, human-machine dialog as well as monolog {--} at both the token and turn levels. In order to examine the generalization of such methods across language pairs and datasets, we analyze 10 different datasets of code-switched text. We extract a variety of character- and word-based text features and pass them into multiple learners, including conditional random fields, logistic regressors and recurrent neural networks. We further examine the efficacy of novel character-level embedding and GloVe features in improving performance and observe that our best-performing text system significantly outperforms a majority vote baseline across language pairs and datasets.},
  comment   = {Language Identification for Code Switching in Dialogue},
  doi       = {10.18653/v1/W18-5009},
  url       = {https://aclanthology.org/W18-5009},
}

@InProceedings{Budzianowski2019,
  author    = {Budzianowski, Pawe{\l} and Vuli{\'c}, Ivan},
  booktitle = {Proceedings of the 3rd Workshop on Neural Generation and Translation},
  title     = {Hello, It{'}s {GPT}-2 - How Can {I} Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems},
  year      = {2019},
  address   = {Hong Kong},
  month     = nov,
  pages     = {15--22},
  publisher = {Association for Computational Linguistics},
  abstract  = {Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework (Wolf et al., 2019) and generative model pre-training (Radford et al., 2019), we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.},
  comment   = {TOD System},
  doi       = {10.18653/v1/D19-5602},
  file      = {:Budzianowski2019.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/D19-5602},
}

@InProceedings{Anikina2019,
  author    = {Anikina, Tatiana and Kruijff-Korbayova, Ivana},
  booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
  title     = {Dialogue Act Classification in Team Communication for Robot Assisted Disaster Response},
  year      = {2019},
  address   = {Stockholm, Sweden},
  month     = sep,
  pages     = {399--410},
  publisher = {Association for Computational Linguistics},
  abstract  = {We present the results we obtained on the classification of dialogue acts in a corpus of human-human team communication in the domain of robot-assisted disaster response. We annotated dialogue acts according to the ISO 24617-2 standard scheme and carried out experiments using the FastText linear classifier as well as several neural architectures, including feed-forward, recurrent and convolutional neural models with different types of embeddings, context and attention mechanism. The best performance was achieved with a {''}Divide {\&} Merge{''} architecture presented in the paper, using trainable GloVe embeddings and a structured dialogue history. This model learns from the current utterance and the preceding context separately and then combines the two generated representations. Average accuracy of 10-fold cross-validation is 79.8{\%}, F-score 71.8{\%}.},
  comment   = {Dialogue Act Classification},
  doi       = {10.18653/v1/W19-5946},
  url       = {https://aclanthology.org/W19-5946},
}

@InProceedings{Ginzburg2019,
  author    = {Ginzburg, Jonathan and Yusupujiang, Zulipiye and Li, Chuyuan and Ren, Kexin and {\L}upkowski, Pawe{\l}},
  booktitle = {Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue},
  title     = {Characterizing the Response Space of Questions: a Corpus Study for {E}nglish and {P}olish},
  year      = {2019},
  address   = {Stockholm, Sweden},
  month     = sep,
  pages     = {320--330},
  publisher = {Association for Computational Linguistics},
  abstract  = {The main aim of this paper is to provide a characterization of the response space for questions using a taxonomy grounded in a dialogical formal semantics. As a starting point we take the typology for responses in the form of questions provided in (Lupkowski and Ginzburg, 2016). This work develops a wide coverage taxonomy for question/question sequences observable in corpora including the BNC, CHILDES, and BEE, as well as formal modelling of all the postulated classes. Our aim is to extend this work to cover all responses to questions. We present the extended typology of responses to questions based on a corpus studies of BNC, BEE and Maptask with include 506, 262, and 467 question/response pairs respectively. We compare the data for English with data from Polish using the Spokes corpus (205 question/response pairs). We discuss annotation reliability and disagreement analysis. We sketch how each class can be formalized using a dialogical semantics appropriate for dialogue management.},
  comment   = {Analysis},
  doi       = {10.18653/v1/W19-5937},
  url       = {https://aclanthology.org/W19-5937},
}

@InProceedings{Balakrishnan2019,
  author    = {Balakrishnan, Anusha and Rao, Jinfeng and Upasani, Kartikeya and White, Michael and Subba, Rajen},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  title     = {Constrained Decoding for Neural {NLG} from Compositional Representations in Task-Oriented Dialogue},
  year      = {2019},
  address   = {Florence, Italy},
  month     = jul,
  pages     = {831--844},
  publisher = {Association for Computational Linguistics},
  abstract  = {Generating fluent natural language responses from structured semantic representations is a critical step in task-oriented conversational systems. Avenues like the E2E NLG Challenge have encouraged the development of neural approaches, particularly sequence-to-sequence (Seq2Seq) models for this problem. The semantic representations used, however, are often underspecified, which places a higher burden on the generation model for sentence planning, and also limits the extent to which generated responses can be controlled in a live system. In this paper, we (1) propose using tree-structured semantic representations, like those used in traditional rule-based NLG systems, for better discourse-level structuring and sentence-level planning; (2) introduce a challenging dataset using this representation for the weather domain; (3) introduce a constrained decoding approach for Seq2Seq models that leverages this representation to improve semantic correctness; and (4) demonstrate promising results on our dataset and the E2E dataset.},
  comment   = {Meaning Representations for NLG in TOD},
  doi       = {10.18653/v1/P19-1080},
  url       = {https://aclanthology.org/P19-1080},
}

@InProceedings{HosseiniAsl2020,
  author    = {Hosseini-Asl, Ehsan and McCann, Bryan and Wu, Chien-Sheng and Yavuz, Semih and Socher, Richard},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  title     = {A Simple Language Model for Task-Oriented Dialogue},
  year      = {2020},
  address   = {Red Hook, NY, USA},
  publisher = {Curran Associates Inc.},
  series    = {NIPS'20},
  abstract  = {Task-oriented dialogue is often decomposed into three tasks: understanding user input, deciding actions, and generating a response. While such decomposition might suggest a dedicated model for each sub-task, we find a simple, unified approach leads to state-of-the-art performance on the MultiWOZ dataset. SimpleTOD is a simple approach to task-oriented dialogue that uses a single, causal language model trained on all sub-tasks recast as a single sequence prediction problem. This allows SimpleTOD to fully leverage transfer learning from pre-trained, open domain, causal language models such as GPT-2. SimpleTOD improves over the prior state-of-the-art in joint goal accuracy for dialogue state tracking, and our analysis reveals robustness to noisy annotations in this setting. SimpleTOD also improves the main metrics used to evaluate action decisions and response generation in an end-to-end setting: inform rate by 8.1 points, success rate by 9.7 points, and combined score by 7.2 points.},
  articleno = {1694},
  comment   = {TOD System},
  file      = {:HosseiniAsl2020.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  isbn      = {9781713829546},
  location  = {Vancouver, BC, Canada},
  numpages  = {13},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/e946209592563be0f01c844ab2170f0c-Abstract.html},
}

@InProceedings{Wu2020,
  author    = {Wu, Chien-Sheng and Hoi, Steven C.H. and Socher, Richard and Xiong, Caiming},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  title     = {{TOD}-{BERT}: Pre-trained Natural Language Understanding for Task-Oriented Dialogue},
  year      = {2020},
  address   = {Online},
  month     = nov,
  pages     = {917--929},
  publisher = {Association for Computational Linguistics},
  abstract  = {The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice. In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling. To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling. We propose a contrastive objective function to simulate the response selection task. Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection. We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.},
  comment   = {NLU for TOD},
  doi       = {10.18653/v1/2020.emnlp-main.66},
  file      = {:Wu2020.pdf:PDF},
  groups    = {TaskOrientedDialogue},
  url       = {https://aclanthology.org/2020.emnlp-main.66},
}

@InProceedings{Bergqvist2020,
  author    = {Bergqvist, Amanda and Manuvinakurike, Ramesh and Karkada, Deepthi and Paetzel, Maike},
  booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {Nontrivial Lexical Convergence in a Geography-Themed Game},
  year      = {2020},
  address   = {1st virtual meeting},
  month     = jul,
  pages     = {209--214},
  publisher = {Association for Computational Linguistics},
  abstract  = {The present study aims to examine the prevalent notion that people entrain to the vocabulary of a dialogue system. Although previous research shows that people will replace their choice of words with simple substitutes, studies using more challenging substitutions are sparse. In this paper, we investigate whether people adapt their speech to the vocabulary of a dialogue system when the system{'}s suggested words are not direct synonyms. 32 participants played a geography-themed game with a remote-controlled agent and were primed by referencing strategies (rather than individual terms) introduced in follow-up questions. Our results suggest that context-appropriate substitutes support convergence and that the convergence has a lasting effect within a dialogue session if the system{'}s wording is more consistent with the norms of the domain than the original wording of the speaker.},
  comment   = {Analysis},
  url       = {https://aclanthology.org/2020.sigdial-1.26},
}

@InProceedings{Shi2020,
  author    = {Shi, Hongjie},
  booktitle = {Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {A Sequence-to-sequence Approach for Numerical Slot-filling Dialog Systems},
  year      = {2020},
  address   = {1st virtual meeting},
  month     = jul,
  pages     = {272--277},
  publisher = {Association for Computational Linguistics},
  abstract  = {Dialog systems capable of filling slots with numerical values have wide applicability to many task-oriented applications. In this paper, we perform a particular case study on the {``}number{\_}of{\_}guests{''} slot-filling in hotel reservation domain, and propose two methods to improve current dialog system model on 1. numerical reasoning performance by training the model to predict arithmetic expressions, and 2. multi-turn question generation by introducing additional context slots. Furthermore, because the proposed methods are all based on an end-to-end trainable sequence-to-sequence (seq2seq) neural model, it is possible to achieve further performance improvement on increasing dialog logs in the future.},
  comment   = {Slot Filling},
  url       = {https://aclanthology.org/2020.sigdial-1.34},
}

@InProceedings{Peng2020,
  author    = {Peng, Baolin and Zhu, Chenguang and Li, Chunyuan and Li, Xiujun and Li, Jinchao and Zeng, Michael and Gao, Jianfeng},
  booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
  title     = {Few-shot Natural Language Generation for Task-Oriented Dialog},
  year      = {2020},
  address   = {Online},
  month     = nov,
  pages     = {172--182},
  publisher = {Association for Computational Linguistics},
  abstract  = {As a crucial component in task-oriented dialog systems, the Natural Language Generation (NLG) module converts a dialog act represented in a semantic form into a response in natural language. The success of traditional template-based or statistical models typically relies on heavily annotated data, which is infeasible for new domains. Therefore, it is pivotal for an NLG system to generalize well with limited labelled data in real applications. To this end, we present FewshotWOZ, the first NLG benchmark to simulate the few-shot learning setting in task-oriented dialog systems. Further, we develop the SC-GPT model. It is pre-trained on a large set of annotated NLG corpus to acquire the controllable generation ability, and fine-tuned with only a few domain-specific labels to adapt to new domains. Experiments on FewshotWOZ and the large Multi-Domain-WOZ datasets show that the proposed SC-GPT significantly outperforms existing methods, measured by various automatic metrics and human evaluations.},
  comment   = {NLG},
  doi       = {10.18653/v1/2020.findings-emnlp.17},
  url       = {https://aclanthology.org/2020.findings-emnlp.17},
}

@InProceedings{Xing2021,
  author    = {Xing, Linzi and Carenini, Giuseppe},
  booktitle = {Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {Improving Unsupervised Dialogue Topic Segmentation with Utterance-Pair Coherence Scoring},
  year      = {2021},
  address   = {Singapore and Online},
  month     = jul,
  pages     = {167--177},
  publisher = {Association for Computational Linguistics},
  abstract  = {Dialogue topic segmentation is critical in several dialogue modeling problems. However, popular unsupervised approaches only exploit surface features in assessing topical coherence among utterances. In this work, we address this limitation by leveraging supervisory signals from the utterance-pair coherence scoring task. First, we present a simple yet effective strategy to generate a training corpus for utterance-pair coherence scoring. Then, we train a BERT-based neural utterance-pair coherence model with the obtained training corpus. Finally, such model is used to measure the topical relevance between utterances, acting as the basis of the segmentation inference. Experiments on three public datasets in English and Chinese demonstrate that our proposal outperforms the state-of-the-art baselines.},
  comment   = {#STUDIS},
  url       = {https://aclanthology.org/2021.sigdial-1.18},
}

@InProceedings{Tian2021,
  author    = {Tian, Ye and Nieradzik, Tim and Jalali, Sepehr and Shiu, Da-shan},
  booktitle = {Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {How does {BERT} process disfluency?},
  year      = {2021},
  address   = {Singapore and Online},
  month     = jul,
  pages     = {208--217},
  publisher = {Association for Computational Linguistics},
  abstract  = {Natural conversations are filled with disfluencies. This study investigates if and how BERT understands disfluency with three experiments: (1) a behavioural study using a downstream task, (2) an analysis of sentence embeddings and (3) an analysis of the attention mechanism on disfluency. The behavioural study shows that without fine-tuning on disfluent data, BERT does not suffer significant performance loss when presented disfluent compared to fluent inputs (exp1). Analysis on sentence embeddings of disfluent and fluent sentence pairs reveals that the deeper the layer, the more similar their representation (exp2). This indicates that deep layers of BERT become relatively invariant to disfluency. We pinpoint attention as a potential mechanism that could explain this phenomenon (exp3). Overall, the study suggests that BERT has knowledge of disfluency structure. We emphasise the potential of using BERT to understand natural utterances without disfluency removal.},
  comment   = {#STUDIS},
  url       = {https://aclanthology.org/2021.sigdial-1.22},
}

@InProceedings{Liao2021,
  author    = {Liao, Ling-Yen and Fares, Tarec},
  booktitle = {Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue},
  title     = {A Practical 2-step Approach to Assist Enterprise Question-Answering Live Chat},
  year      = {2021},
  address   = {Singapore and Online},
  month     = jul,
  pages     = {457--468},
  publisher = {Association for Computational Linguistics},
  abstract  = {Live chat in customer service platforms is critical for serving clients online. For multi-turn question-answering live chat, typical Question Answering systems are single-turn and focus on factoid questions; alternatively, modeling as goal-oriented dialogue limits us to narrower domains. Motivated by these challenges, we develop a new approach based on a framework from a different discipline: Community Question Answering. Specifically, we opt to divide and conquer the task into two sub-tasks: (1) Question-Question Similarity, where we gain more than 9{\%} absolute improvement in F1 over baseline; and (2) Answer Utterances Extraction, where we achieve a high F1 score of 87{\%} for this new sub-task. Further, our user engagement metrics reveal how the enterprise support representatives benefit from the 2-step approach we deployed to production.},
  comment   = {Application, IR Methods},
  url       = {https://aclanthology.org/2021.sigdial-1.48},
}

@Article{Yang2021,
  author    = {Yunyi Yang and Yunhao Li and Xiaojun Quan},
  journal   = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  title     = {{UBAR}: Towards Fully End-to-End Task-Oriented Dialog System with {GPT}-2},
  year      = {2021},
  month     = {may},
  number    = {16},
  pages     = {14230--14238},
  volume    = {35},
  comment   = {TOD System},
  doi       = {10.1609/aaai.v35i16.17674},
  publisher = {Association for the Advancement of Artificial Intelligence ({AAAI})},
}
